{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal algorithms\n",
    "\n",
    "In this notebook, we code our proximal optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Proximal Gradient algorithm\n",
    "\n",
    "For minimizing a function $F:\\mathbb{R}^n \\to \\mathbb{R}$ equal to $f+g$ where $f$ is differentiable and the $\\mathbf{prox}$ of $g$ is known, given:\n",
    "* the function to minimize `F`\n",
    "* a 1st order oracle for $f$ `f_grad` \n",
    "* a proximity operator for $g$ `g_prox` \n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "* a display boolean variable `PRINT` \n",
    "\n",
    "these algorithms perform iterations of the form\n",
    "$$ x_{k+1} = \\mathbf{prox}_{\\gamma g}\\left( x_k - \\gamma \\nabla f(x_k) \\right) $$\n",
    "where $\\gamma$ is a stepsize to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> Fill the function below with the proximal gradient algorithm.\n",
    "\n",
    "> How would you implement the precision stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def proximal_gradient_algorithm(F , f_grad , g_prox , x0 , step , PREC , ITE_MAX , PRINT ):\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "    if PRINT:\n",
    "        print(\"------------------------------------\\n Proximal gradient algorithm\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        x_prev = x\n",
    "        g = f_grad(x)\n",
    "        x = g_prox(x - step*g , step)  #######  ITERATION\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "        # Stop Condition\n",
    "        if np.abs(np.linalg.norm(x)-np.linalg.norm(x_prev)) < PREC:\n",
    "            break\n",
    "\n",
    "    t_e =  timeit.default_timer()\n",
    "    if PRINT:\n",
    "        print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,F(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient \n",
    "\n",
    "\n",
    "In the following, instead of considering $f$ as a whole, we will use its structure \n",
    "$$ f(x) := \\frac{1}{m}\\sum_{i=1}^m f_i(x)$$\n",
    "\n",
    "> Implement the gradient related to $f_i$, related to one example, in ``[logistic_regression_student]``\n",
    "\n",
    "With this structure a popular minimization algorithm is the *stochastic gradient algorithm* which writes as follows:\n",
    "* Select uniformly $i$ in $1,..,m$\n",
    "* $x_{k+1} = \\mathbf{prox}_{\\gamma g}\\left( x_k - \\gamma_k \\nabla f_i(x_k) \\right) $\n",
    "\n",
    "> Implement this algorithm with a stepsize vanishing as $1/k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def stochastic_gradient_descent(F , f_grad , f_grad_i , g_prox , x0 , m, step , PREC , ITE_MAX , PRINT ):\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "    if PRINT:\n",
    "        print(\"------------------------------------\\n Stochastic gradient descent algorithm\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        x_prev = x\n",
    "        \n",
    "        gamma = (step/(k+1))\n",
    "        \n",
    "        i = np.random.randint(1,m) # choose uniformly i in 1, ..., nbr of realisation = final_grades.size\n",
    "        \n",
    "        x = x - gamma * f_grad_i(x,i)\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "        ## Stop Condition \n",
    "        #if np.abs(np.linalg.norm(x)-np.linalg.norm(x_prev)) < PREC:\n",
    "        #    break\n",
    "        \n",
    "        # Cesaro stop criterion\n",
    "        if np.allclose(x, cesaro(x_tab[:-1]), rtol=PREC):\n",
    "            break\n",
    "\n",
    "    t_e =  timeit.default_timer()\n",
    "    if PRINT:\n",
    "        print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,F(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saga(F , f_grad , f_grad_i , g_prox , x0 , m, step , PREC , ITE_MAX ,  PRINT):\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "    fi_tab = np.copy(f_grad_i)\n",
    "    phi = x0\n",
    "    \n",
    "    if PRINT:\n",
    "        print(\"------------------------------------\\n SAGA \\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        x_prev = x\n",
    "        fi_prev = f_grad_i(x_prev)\n",
    "        \n",
    "        gamma = (step/(k+1))\n",
    "        \n",
    "        i = np.random.randint(1,m) # choose uniformly i in 1, ..., nbr of realisation = final_grades.size\n",
    "        \n",
    "        print(\"a\")\n",
    "        \n",
    "        w = x - gamma * (f_grad_i(x,i) - fi_prev + cesaro(fi_tab))\n",
    "                         \n",
    "        print(\"a\")\n",
    "                         \n",
    "        x = prox(x, w, F, gamma)\n",
    "        \n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "        fi_tab = np.vstack((fi_tab,f_grad_i(x)))\n",
    "        \n",
    "        # Cesaro stop criterion\n",
    "        if np.allclose(x, cesaro(x_tab[:-1]), rtol=PREC):\n",
    "            break\n",
    "        \n",
    "    t_e =  timeit.default_timer()\n",
    "    if PRINT:\n",
    "        print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,F(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cesaro mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cesaro(X):\n",
    "    m = np.shape(X)[0] # number of iterates\n",
    "    \n",
    "    return np.sum(X,axis=0)/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox(x, y, h, gamma):\n",
    "    return (np.argmin(h(x) + np.linalg.norm(x-y)^2)/(2*gamma))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
